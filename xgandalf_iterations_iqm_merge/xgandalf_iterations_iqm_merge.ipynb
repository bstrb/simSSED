{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the CrystFEL-based processing\n",
    "\n",
    "This notebook comprises a workflow using CrystFEL-based tools combined with original scripts. Below is a step-by-step guide:\n",
    "\n",
    "1. **Run Indexamajig** (`gandalf_iterator`)  \n",
    "   - Perform peakfinding, indexing, and integration for each HDF5 file in the specified folder, varying beam center coordinates on a grid withing a given radius.\n",
    "\n",
    "2. **Evaluate IQM** (`automate_evaluation`)  \n",
    "   - Parse stream files for indexing quality metrics (IQMs), apply weights, and identify the best results.\n",
    "\n",
    "3. **Merge** (`merge`)  \n",
    "   - Merge the best result stream file to refine cell parameters and symmetry.\n",
    "\n",
    "4. **Convert**  \n",
    "   - **to `.hkl`** for ShelX (`convert_hkl_crystfel_to_shelx`).  \n",
    "   - **to `.mtz`** for downstream crystallographic tools (`convert_hkl_to_mtz`).\n",
    "\n",
    "Please ensure that preprocessing (peak finding, center finding and center refinement) has been done and that you have the required packages and environment set up (CrystFEL, Python packages, etc.) before proceeding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Indexamajig with options for peakfinding, indexing and integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gandalf_radial_iterator import gandalf_iterator\n",
    "\n",
    "geomfile_path = \"/Users/xiaodong/Desktop/simulations/LTA/LTAsim.geom\"       # .geom file\n",
    "cellfile_path = \"/Users/xiaodong/Desktop/simulations/LTA/LTA.cell\"          # .cell file\n",
    "\n",
    "input_path =   \"/Users/xiaodong/Desktop/simulations/LTA/simulation-24\"      # .h5 folder will also be output folder\n",
    "\n",
    "output_file_base = \"LTA\"    # output files will be named output_file_base_xcoord_ycoord.h5\n",
    "\n",
    "num_threads = 8             # number of CPU threads to use\n",
    "x, y = 512.5, 512.5         # initial beam center from where iterations will start\n",
    "\n",
    "\"\"\"Define the grid and maximum radius for iterations.\n",
    "As example max_radius = 1, step = 0.2 will give 81 iterations.\n",
    "Iterations will start at the center and move radially outwards.\n",
    "\"\"\"\n",
    "max_radius = 1              # maximum radius in pixels\n",
    "step = 0.2                  # grid granularity in pixels\n",
    "\n",
    "extra_flags=[\n",
    "# PEAKFINDING\n",
    "\"--no-revalidate\",\n",
    "\"--no-half-pixel-shift\",\n",
    "\"--peaks=cxi\", \n",
    "\"--min-peaks=15\",\n",
    "# INDEXING\n",
    "\"--indexing=xgandalf\",\n",
    "\"--tolerance=10,10,10,5\",\n",
    "\"--no-refine\",\n",
    "\"--xgandalf-sampling-pitch=5\",\n",
    "\"--xgandalf-grad-desc-iterations=1\",\n",
    "\"--xgandalf-tolerance=0.02\",\n",
    "# INTEGRATION\n",
    "\"--integration=rings\",\n",
    "\"--int-radius=4,5,9\",\n",
    "\"--fix-profile-radius=70000000\",\n",
    "# OUTPUT\n",
    "\"--no-non-hits-in-stream\",\n",
    "]\n",
    "\n",
    "\"\"\"Examples of extra flags(see crystfel documentation https://www.desy.de/~twhite/crystfel/manual-indexamajig.html):\"\"\"\n",
    "\n",
    "\"\"\" Basic options\n",
    "\"--highres=n\",\n",
    "\"--no-image-data\",\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Peakfinding\n",
    "\"--peaks=cxi\",\n",
    "\"--peak-radius=inner,middle,outer\",\n",
    "\"--min-peaks=n\",\n",
    "\"--median-filter=n\",\n",
    "\"--filter-noise\",\n",
    "\"--no-revalidate\",\n",
    "\"--no-half-pixel-shift\",\n",
    "\n",
    "\"--peaks=peakfinder9\",\n",
    "\"--min-snr=1\",\n",
    "\"--min-snr-peak-pix=6\",\n",
    "\"--min-snr-biggest-pix=1\",\n",
    "\"--min-sig=9\",\n",
    "\"--min-peak-over-neighbour=5\",\n",
    "\"--local-bg-radius=5\",\n",
    "\n",
    "\"--peaks=peakfinder8\",\n",
    "\"--threshold=45\",\n",
    "\"--min-snr=3\",\n",
    "\"--min-pix-count=3\",\n",
    "\"--max-pix-count=500\",\n",
    "\"--local-bg-radius=9\",\n",
    "\"--min-res=30\",\n",
    "\"--max-res=500\",\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Indexing\n",
    "\"--indexing=xgandalf\",\n",
    "\n",
    "\"--tolerance=tol\"\n",
    "\"--no-check-cell\",\n",
    "\"--no-check-peaks\",\n",
    "\"--multi\",\n",
    "\"--no-retry\",\n",
    "\"--no-refine\",\n",
    "\n",
    "\"--xgandalf-sampling-pitch=n\"\n",
    "\"--xgandalf-grad-desc-iterations=n\"\n",
    "\"--xgandalf-tolerance=n\"\n",
    "\"--xgandalf-no-deviation-from-provided-cell\"\n",
    "\"--xgandalf-max-lattice-vector-length=n\"\n",
    "\"--xgandalf-min-lattice-vector-length=n\"\n",
    "\"--xgandalf-max-peaks=n\"\n",
    "\"--xgandalf-fast-execution\"\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Integration\n",
    "\"--fix-profile-radius=n\",\n",
    "\"--fix-divergence=n\",\n",
    "\"--integration=rings\",\n",
    "\"--int-radius=4,5,10\",\n",
    "\"--push-res=n\",\n",
    "\"--overpredict\",\n",
    "\"--cell-parameters-only\",\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Output\n",
    "\"--no-non-hits-in-stream\",\n",
    "\"--no-peaks-in-stream\",\n",
    "\"--no-refls-in-stream\",\n",
    "\"--serial-offset\n",
    "\"\"\"\n",
    "\n",
    "gandalf_iterator(x, y, geomfile_path, cellfile_path, input_path, output_file_base, num_threads, max_radius=max_radius, step=step, extra_flags=extra_flags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the IQM with chosen weights for all frames across all index results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating multiple stream files with weights: (1, 1, 1, 1, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks in LTA_-513.3_-512.1.stream: 100%|██████████| 100/100 [00:04<00:00, 20.63chunk/s]\n",
      "Processing chunks in LTA_-513.3_-513.1.stream: 100%|██████████| 100/100 [00:05<00:00, 19.62chunk/s]\n",
      "Processing chunks in LTA_-513.1_-511.9.stream: 100%|██████████| 100/100 [00:05<00:00, 17.96chunk/s]\n",
      "Processing chunks in LTA_-512.9_-512.1.stream: 100%|██████████| 100/100 [00:08<00:00, 11.28chunk/s]\n",
      "Processing chunks in LTA_-512.9_-513.1.stream: 100%|██████████| 100/100 [00:10<00:00,  9.93chunk/s]\n",
      "Processing chunks in LTA_-512.7_-512.9.stream: 100%|██████████| 100/100 [00:11<00:00,  8.45chunk/s]\n",
      "Processing chunks in LTA_-512.3_-512.9.stream: 100%|██████████| 100/100 [00:12<00:00,  8.28chunk/s]\n",
      "Processing chunks in LTA_-512.7_-512.5.stream: 100%|██████████| 100/100 [00:12<00:00,  7.79chunk/s]\n",
      "Processing chunks in LTA_-513.3_-512.3.stream: 100%|██████████| 100/100 [00:07<00:00, 12.84chunk/s]\n",
      "Processing chunks in LTA_-512.1_-511.7.stream: 100%|██████████| 100/100 [00:04<00:00, 21.95chunk/s]\n",
      "Processing chunks in LTA_-512.9_-513.3.stream: 100%|██████████| 100/100 [00:06<00:00, 16.22chunk/s]\n",
      "Processing chunks in LTA_-512.3_-512.5.stream: 100%|██████████| 100/100 [00:13<00:00,  7.51chunk/s]\n",
      "Processing chunks in LTA_-512.7_-512.7.stream: 100%|██████████| 100/100 [00:13<00:00,  7.60chunk/s]\n",
      "Processing chunks in LTA_-512.3_-513.3.stream: 100%|██████████| 100/100 [00:07<00:00, 13.25chunk/s]\n",
      "Processing chunks in LTA_-512.3_-512.7.stream: 100%|██████████| 100/100 [00:12<00:00,  7.79chunk/s]\n",
      "Processing chunks in LTA_-512.9_-512.3.stream: 100%|██████████| 100/100 [00:12<00:00,  7.87chunk/s]\n",
      "Processing chunks in LTA_-512.5_-511.7.stream: 100%|██████████| 100/100 [00:06<00:00, 16.32chunk/s]\n",
      "Processing chunks in LTA_-513.3_-512.7.stream: 100%|██████████| 100/100 [00:06<00:00, 14.41chunk/s]\n",
      "Processing chunks in LTA_-512.3_-512.3.stream: 100%|██████████| 100/100 [00:11<00:00,  8.53chunk/s]\n",
      "Processing chunks in LTA_-512.7_-513.3.stream: 100%|██████████| 100/100 [00:06<00:00, 14.98chunk/s]\n",
      "Processing chunks in LTA_-511.5_-512.5.stream: 100%|██████████| 100/100 [00:05<00:00, 16.79chunk/s]\n",
      "Processing chunks in LTA_-512.7_-512.3.stream: 100%|██████████| 100/100 [00:12<00:00,  8.13chunk/s]\n",
      "Processing chunks in LTA_-512.1_-511.9.stream: 100%|██████████| 100/100 [00:06<00:00, 15.67chunk/s]\n",
      "Processing chunks in LTA_-512.9_-512.7.stream: 100%|██████████| 100/100 [00:12<00:00,  8.24chunk/s]\n",
      "Processing chunks in LTA_-512.9_-512.9.stream: 100%|██████████| 100/100 [00:11<00:00,  8.90chunk/s]\n",
      "Processing chunks in LTA_-512.3_-513.1.stream: 100%|██████████| 100/100 [00:08<00:00, 11.75chunk/s]\n",
      "Processing chunks in LTA_-512.3_-512.1.stream: 100%|██████████| 100/100 [00:09<00:00, 11.08chunk/s]\n",
      "Processing chunks in LTA_-512.5_-511.5.stream: 100%|██████████| 100/100 [00:03<00:00, 28.01chunk/s]\n",
      "Processing chunks in LTA_-512.9_-512.5.stream: 100%|██████████| 100/100 [00:12<00:00,  7.91chunk/s]\n",
      "Processing chunks in LTA_-511.9_-511.9.stream: 100%|██████████| 100/100 [00:04<00:00, 21.88chunk/s]\n",
      "Processing chunks in LTA_-513.3_-512.5.stream: 100%|██████████| 100/100 [00:07<00:00, 13.41chunk/s]\n",
      "Processing chunks in LTA_-512.7_-513.1.stream: 100%|██████████| 100/100 [00:09<00:00, 10.86chunk/s]\n",
      "Processing chunks in LTA_-513.3_-512.9.stream: 100%|██████████| 100/100 [00:07<00:00, 13.01chunk/s]\n",
      "Processing chunks in LTA_-512.5_-511.9.stream: 100%|██████████| 100/100 [00:07<00:00, 13.37chunk/s]\n",
      "Processing chunks in LTA_-511.7_-512.7.stream: 100%|██████████| 100/100 [00:07<00:00, 13.03chunk/s]\n",
      "Processing chunks in LTA_-512.7_-512.1.stream: 100%|██████████| 100/100 [00:11<00:00,  8.75chunk/s]\n",
      "Processing chunks in LTA_-511.9_-512.3.stream: 100%|██████████| 100/100 [00:07<00:00, 13.48chunk/s]\n",
      "Processing chunks in LTA_-513.1_-512.7.stream: 100%|██████████| 100/100 [00:10<00:00,  9.95chunk/s]\n",
      "Processing chunks in LTA_-512.7_-511.7.stream: 100%|██████████| 100/100 [00:04<00:00, 22.86chunk/s]\n",
      "Processing chunks in LTA_-511.9_-513.3.stream: 100%|██████████| 100/100 [00:06<00:00, 14.70chunk/s]\n",
      "Processing chunks in LTA_-512.5_-513.3.stream: 100%|██████████| 100/100 [00:06<00:00, 16.58chunk/s]\n",
      "Processing chunks in LTA_-512.3_-511.7.stream: 100%|██████████| 100/100 [00:05<00:00, 19.83chunk/s]\n",
      "Processing chunks in LTA_-512.1_-513.3.stream: 100%|██████████| 100/100 [00:07<00:00, 12.66chunk/s]\n",
      "Processing chunks in LTA_-512.7_-511.9.stream: 100%|██████████| 100/100 [00:05<00:00, 17.75chunk/s]\n",
      "Processing chunks in LTA_-512.5_-512.3.stream: 100%|██████████| 100/100 [00:11<00:00,  8.44chunk/s]\n",
      "Processing chunks in LTA_-511.9_-512.1.stream: 100%|██████████| 100/100 [00:06<00:00, 16.56chunk/s]\n",
      "Processing chunks in LTA_-512.5_-513.1.stream: 100%|██████████| 100/100 [00:08<00:00, 11.58chunk/s]\n",
      "Processing chunks in LTA_-512.1_-512.3.stream: 100%|██████████| 100/100 [00:09<00:00, 10.04chunk/s]\n",
      "Processing chunks in LTA_-513.1_-512.9.stream: 100%|██████████| 100/100 [00:10<00:00,  9.89chunk/s]\n",
      "Processing chunks in LTA_-512.5_-512.1.stream: 100%|██████████| 100/100 [00:10<00:00,  9.32chunk/s]\n",
      "Processing chunks in LTA_-512.3_-511.9.stream: 100%|██████████| 100/100 [00:05<00:00, 17.10chunk/s]\n",
      "Processing chunks in LTA_-513.5_-512.5.stream: 100%|██████████| 100/100 [00:05<00:00, 17.07chunk/s]\n",
      "Processing chunks in LTA_-511.9_-513.1.stream: 100%|██████████| 100/100 [00:07<00:00, 13.08chunk/s]\n",
      "Processing chunks in LTA_-513.1_-512.5.stream: 100%|██████████| 100/100 [00:08<00:00, 11.28chunk/s]\n",
      "Processing chunks in LTA_-512.1_-512.1.stream: 100%|██████████| 100/100 [00:06<00:00, 14.84chunk/s]\n",
      "Processing chunks in LTA_-512.5_-513.5.stream: 100%|██████████| 100/100 [00:04<00:00, 21.55chunk/s]\n",
      "Processing chunks in LTA_-512.1_-513.1.stream: 100%|██████████| 100/100 [00:08<00:00, 11.41chunk/s]\n",
      "Processing chunks in LTA_-511.7_-512.5.stream: 100%|██████████| 100/100 [00:08<00:00, 12.36chunk/s]\n",
      "Processing chunks in LTA_-511.7_-512.9.stream: 100%|██████████| 100/100 [00:07<00:00, 12.74chunk/s]\n",
      "Processing chunks in LTA_-513.3_-511.9.stream: 100%|██████████| 100/100 [00:04<00:00, 20.41chunk/s]\n",
      "Processing chunks in LTA_-512.1_-512.5.stream: 100%|██████████| 100/100 [00:10<00:00,  9.53chunk/s]\n",
      "Processing chunks in LTA_-512.1_-512.9.stream: 100%|██████████| 100/100 [00:11<00:00,  8.64chunk/s]\n",
      "Processing chunks in LTA_-511.9_-512.5.stream: 100%|██████████| 100/100 [00:08<00:00, 11.48chunk/s]\n",
      "Processing chunks in LTA_-513.1_-512.1.stream: 100%|██████████| 100/100 [00:07<00:00, 13.88chunk/s]\n",
      "Processing chunks in LTA_-513.1_-513.1.stream: 100%|██████████| 100/100 [00:08<00:00, 11.18chunk/s]\n",
      "Processing chunks in LTA_-512.5_-512.5.stream: 100%|██████████| 100/100 [00:12<00:00,  8.14chunk/s]\n",
      "Processing chunks in LTA_-511.9_-512.9.stream: 100%|██████████| 100/100 [00:08<00:00, 11.70chunk/s]\n",
      "Processing chunks in LTA_-511.7_-512.3.stream: 100%|██████████| 100/100 [00:04<00:00, 22.30chunk/s]\n",
      "Processing chunks in LTA_-512.9_-511.7.stream: 100%|██████████| 100/100 [00:04<00:00, 21.22chunk/s]\n",
      "Processing chunks in LTA_-511.7_-512.1.stream: 100%|██████████| 100/100 [00:05<00:00, 17.11chunk/s]\n",
      "Processing chunks in LTA_-512.9_-511.9.stream: 100%|██████████| 100/100 [00:06<00:00, 14.36chunk/s]\n",
      "Processing chunks in LTA_-513.1_-513.3.stream: 100%|██████████| 100/100 [00:05<00:00, 16.96chunk/s]\n",
      "Processing chunks in LTA_-512.5_-512.9.stream: 100%|██████████| 100/100 [00:10<00:00,  9.41chunk/s]\n",
      "Processing chunks in LTA_-513.1_-512.3.stream: 100%|██████████| 100/100 [00:06<00:00, 14.70chunk/s]\n",
      "Processing chunks in LTA_-512.1_-512.7.stream: 100%|██████████| 100/100 [00:08<00:00, 11.26chunk/s]\n",
      "Processing chunks in LTA_-511.9_-512.7.stream: 100%|██████████| 100/100 [00:06<00:00, 16.04chunk/s]\n",
      "Processing chunks in LTA_-512.5_-512.7.stream: 100%|██████████| 100/100 [00:07<00:00, 13.72chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined metrics CSV written to /Users/xiaodong/Desktop/simulations/LTA/simulation-24/xgandalf_iterations_max_radius_1_step_0.2/metric_values_IQM_1_1_1_1_1_1.csv\n",
      "Best results stream file written to /Users/xiaodong/Desktop/simulations/LTA/simulation-24/xgandalf_iterations_max_radius_1_step_0.2/merged_IQM_1_1_1_1_1_1.stream\n"
     ]
    }
   ],
   "source": [
    "from automate_evaluation import automate_evaluation\n",
    "\n",
    "# Enter folder with stream file results from indexamajig. \n",
    "# Note that ALL stream files in the folder will be processed.\n",
    "stream_file_folder = \"/Users/xiaodong/Desktop/simulations/LTA/simulation-24/xgandalf_iterations_max_radius_1_step_0.2\" \n",
    "\n",
    "weights_list = [\n",
    "    (1, 1, 1, 1, 1, 1)\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Each weight corresponds to one of the six metrics used in calculating the combined IQM value.\n",
    "The combined IQM is computed by first normalizing each metric across all stream files, then \n",
    "multiplying each normalized metric by its assigned weight, and finally summing the results.\n",
    "The order (or keys) of the weights must match the following metrics:\n",
    "\n",
    "- 'weighted_rmsd'\n",
    "- 'fraction_outliers'\n",
    "- 'length_deviation'\n",
    "- 'angle_deviation'\n",
    "- 'peak_ratio'\n",
    "- 'percentage_indexed'\n",
    "\n",
    "Multiple weight combinations can be specified if needed.\n",
    "\"\"\"\n",
    "\n",
    "automate_evaluation(stream_file_folder, weights_list, indexing_tolerance=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74321082d1524acb9c6d61611868af21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=1.528445243520403, description='combined_metric ≤', max=1.528445243520403, min=-7.4551270789…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf7765390e647888d554f3033691f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=-0.08409381599086142, description='weighted_rmsd ≤', max=-0.08409381599086142, min=-0.390105…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfdccca8c4642fc9b8a473af3e3db73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.8743852735883026, description='fraction_outliers ≤', max=0.8743852735883026, min=-1.674371…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf27e6e089bf458b81793a074fd471a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=2.066147638068251, description='length_deviation ≤', max=2.066147638068251, min=-2.193098975…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4650a704f44b13ab9a811b631c14ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=1.4546725967396552, description='angle_deviation ≤', max=1.4546725967396552, min=-2.30275985…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf221c8750e444ab35347968c79611b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=1.0819393459096938, description='peak_ratio ≤', max=1.0819393459096938, min=-2.7932586938107…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df13394aa9b408896a3223f73e5a736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.007790796995297262, description='percentage_unindexed ≤', max=0.007790796995297262, min=-0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a0649adde54f66a293afe84e109fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Apply Threshold & Show Histograms', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a855d07905045d59d2490de1f4f178f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from interactive_iqm import (\n",
    "    read_metric_csv, \n",
    "    select_best_results_by_event, \n",
    "    get_metric_ranges,\n",
    "    filter_rows,\n",
    "    write_filtered_stream\n",
    ")\n",
    "\n",
    "\n",
    "# 1) Set the paths to your CSV and your existing \"best results\" .stream\n",
    "CSV_PATH = \"/Users/xiaodong/Desktop/simulations/LTA/simulation-24/xgandalf_iterations_max_radius_1_step_0.2/metric_values_IQM_1_1_1_1_1_1.csv\"\n",
    "BEST_STREAM_PATH = \"/Users/xiaodong/Desktop/simulations/LTA/simulation-24/xgandalf_iterations_max_radius_1_step_0.2/merged_IQM_1_1_1_1_1_1.stream\"\n",
    "FILTERED_STREAM_PATH = \"/Users/xiaodong/Desktop/simulations/LTA/simulation-24/xgandalf_iterations_max_radius_1_step_0.2/merged_IQM_1_1_1_1_1_1_filtered.stream\"\n",
    "\n",
    "# 2) Read the CSV (grouped by event string)\n",
    "grouped_data = read_metric_csv(CSV_PATH, group_by_event=True)\n",
    "\n",
    "# 3) If your CSV has multiple rows per event_number, pick \"best\" by combined_metric\n",
    "best_rows = select_best_results_by_event(grouped_data, sort_metric='combined_metric')\n",
    "\n",
    "# 4) Find the min/max range of each metric in best_rows\n",
    "metrics_in_order = [\n",
    "    'combined_metric', 'weighted_rmsd', 'fraction_outliers',\n",
    "    'length_deviation', 'angle_deviation', 'peak_ratio',\n",
    "    'percentage_unindexed'\n",
    "]\n",
    "ranges_dict = get_metric_ranges(best_rows, metrics=metrics_in_order)\n",
    "\n",
    "# 5) Create sliders automatically based on actual data range\n",
    "metric_sliders = {}\n",
    "\n",
    "def create_slider(metric_name, min_val, max_val):\n",
    "    # We'll do a ≤ filter, so let's default the slider to the max for \"include all\"\n",
    "    default_val = max_val\n",
    "    step = (max_val - min_val) / 100.0 if max_val != min_val else 0.01\n",
    "    \n",
    "    slider = widgets.FloatSlider(\n",
    "        value=default_val,\n",
    "        min=min_val,\n",
    "        max=max_val,\n",
    "        step=step,\n",
    "        description=f'{metric_name} ≤'\n",
    "    )\n",
    "    return slider\n",
    "\n",
    "for metric in metrics_in_order:\n",
    "    mn, mx = ranges_dict[metric]\n",
    "    metric_sliders[metric] = create_slider(metric, mn, mx)\n",
    "\n",
    "# 6) Create a button to run the filtering & show histograms\n",
    "filter_button = widgets.Button(description=\"Apply Threshold & Show Histograms\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def make_histograms(rows, metrics=None, bins=20):\n",
    "    \"\"\"Simple function to plot separate histograms (one figure per metric).\"\"\"\n",
    "    if metrics is None:\n",
    "        metrics = metrics_in_order\n",
    "    for m in metrics:\n",
    "        values = [r[m] for r in rows]\n",
    "        plt.figure()\n",
    "        plt.hist(values, bins=bins)\n",
    "        plt.title(f'Histogram of {m}')\n",
    "        plt.xlabel(m)\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "\n",
    "def on_filter_clicked(_):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "\n",
    "        # Build threshold dict from the current slider values\n",
    "        thresholds = {m: metric_sliders[m].value for m in metrics_in_order}\n",
    "\n",
    "        # Filter the best_rows\n",
    "        filtered = filter_rows(best_rows, thresholds)\n",
    "\n",
    "        print(f\"Filtering... {len(best_rows)} events -> {len(filtered)} pass thresholds.\\n\")\n",
    "        \n",
    "        if filtered:\n",
    "            make_histograms(filtered, metrics_in_order)\n",
    "        else:\n",
    "            print(\"No events passed the thresholds, skipping histograms.\")\n",
    "\n",
    "        # Write a new .stream containing only filtered events\n",
    "        write_filtered_stream(filtered, BEST_STREAM_PATH, FILTERED_STREAM_PATH)\n",
    "\n",
    "filter_button.on_click(on_filter_clicked)\n",
    "\n",
    "# 7) Display everything\n",
    "all_widgets = list(metric_sliders.values()) + [filter_button, output_area]\n",
    "display(*all_widgets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge the best results stream file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running partialator for stream file: /Users/xiaodong/Desktop/simulations/LTA/simulation-24/xgandalf_iterations_max_radius_1_step_0.2/merged_IQM_1_1_1_1_1_1.stream\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partialator Progress:   0%|          | 0/7 [00:00<?, ?Residual/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partialator Progress: 100%|██████████| 7/7 [00:01<00:00,  6.78Residual/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partialator completed for stream file: /Users/xiaodong/Desktop/simulations/LTA/simulation-24/xgandalf_iterations_max_radius_1_step_0.2/merged_IQM_1_1_1_1_1_1.stream\n",
      "Merging done. Results are in: /Users/xiaodong/Desktop/simulations/LTA/simulation-24/xgandalf_iterations_max_radius_1_step_0.2/merged_IQM_1_1_1_1_1_1_merge_5_iter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from merge import merge\n",
    "\n",
    "stream_file = \"/Users/xiaodong/Desktop/simulations/LTA/simulation-24/xgandalf_iterations_max_radius_1_step_0.2/merged_IQM_1_1_1_1_1_1.stream\"\n",
    "pointgroup = \"m-3m\"\n",
    "num_threads = 24\n",
    "iterations = 5\n",
    "\n",
    "output_dir = merge(\n",
    "    stream_file,\n",
    "    pointgroup=pointgroup,\n",
    "    num_threads=num_threads,\n",
    "    iterations=iterations,\n",
    ")\n",
    "\n",
    "if output_dir is not None:\n",
    "    print(\"Merging done. Results are in:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to shelx compatible .hkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Converting crystfel.hkl to shelx.hkl in directory: /Users/xiaodong/Desktop/simulations/LTA/simulation-24/xgandalf_iterations_max_radius_1_step_0.2/merged_IQM_1_1_1_1_1_1_merge_5_iter\n",
      "[INFO] Conversion to shelx.hkl completed successfully in: /Users/xiaodong/Desktop/simulations/LTA/simulation-24/xgandalf_iterations_max_radius_1_step_0.2/merged_IQM_1_1_1_1_1_1_merge_5_iter/shelx\n"
     ]
    }
   ],
   "source": [
    "from convert_hkl_crystfel_to_shelx import convert_hkl_crystfel_to_shelx \n",
    "output_dir = \"/Users/xiaodong/Desktop/simulations/LTA/simulation-24/xgandalf_iterations_max_radius_1_step_0.2/merged_IQM_1_1_1_1_1_1_merge_5_iter\"\n",
    "convert_hkl_crystfel_to_shelx(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to mtz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Converting crystfel.hkl to output.mtz in directory: /Users/xiaodong/Desktop/simulations/LTA/simulation-24/xgandalf_iterations_max_radius_1_step_0.2/merged_IQM_1_1_1_1_1_1_merge_5_iter\n",
      "[INFO] Conversion to output.mtz completed successfully in: /Users/xiaodong/Desktop/simulations/LTA/simulation-24/xgandalf_iterations_max_radius_1_step_0.2/merged_IQM_1_1_1_1_1_1_merge_5_iter\n"
     ]
    }
   ],
   "source": [
    "from convert_hkl_to_mtz import convert_hkl_to_mtz\n",
    "cellfile_path = \"/Users/xiaodong/Desktop/simulations/LTA/LTA.cell\"  # If defined above comment out this line\n",
    "convert_hkl_to_mtz(output_dir, cellfile_path=cellfile_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyxem-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
