<!-- Adaptive Sampling Strategies for Center Tracking   -->
When measuring a drifting diffraction center, it’s inefficient to recompute the center every frame. **Adaptive sampling** addresses this by deciding *when* to take an expensive measurement based on statistical cues. Rather than a fixed rate, measurements are triggered only as needed. For example, an *event-triggered* strategy uses a threshold on predicted change or uncertainty: if the drift remains small or predictable, the algorithm skips updates, and if the estimated drift deviates beyond a set threshold, a new center calculation is triggered ([Event-Triggered Kalman Filter and Its Performance Analysis](https://www.mdpi.com/1424-8220/23/4/2202#:~:text=accurately%20set,effectiveness%20of%20the%20proposed%20algorithm)) ([Event-Triggered Kalman Filter and Its Performance Analysis](https://www.mdpi.com/1424-8220/23/4/2202#:~:text=As%20early%20as%201983%2C%20Ho,sensor%20nodes%20that%20can%20be)). This approach effectively implements a **“send-on-delta”** rule – only sampling when the position change exceeds some delta – which avoids redundant calculations during stable periods ([Event-Triggered Kalman Filter and Its Performance Analysis](https://www.mdpi.com/1424-8220/23/4/2202#:~:text=As%20early%20as%201983%2C%20Ho,sensor%20nodes%20that%20can%20be)). By tuning the threshold to the required accuracy, one can balance update frequency against drift tolerance.  

- **Advantages:** Adaptive schemes significantly reduce computation and data throughput by *only updating when necessary*, without sacrificing accuracy ([Event-Triggered Kalman Filter and Its Performance Analysis](https://www.mdpi.com/1424-8220/23/4/2202#:~:text=accurately%20set,effectiveness%20of%20the%20proposed%20algorithm)). They naturally handle periods of stability (skipping unnecessary work) and focus resources on intervals of rapid change.  
- **Limitations:** They require a reliable triggering criterion. If the threshold is set too high, the tracker may miss subtle drift until it accumulates, leading to larger corrections; too low and it approaches a fixed-rate sampling (losing the benefit). Designing triggers often assumes a statistical model for changes (e.g. Gaussian noise) ([Event-Triggered Kalman Filter and Its Performance Analysis](https://www.mdpi.com/1424-8220/23/4/2202#:~:text=In%20estimation%20of%20linear%20systems%2C,triggered)), so unexpected drift patterns might still slip through. Additionally, event-triggered methods add complexity in determining and adjusting the threshold or confidence level for triggering.  

# Kalman Filtering for Random-Walk Drift  
The **Kalman filter** is a classic algorithm for iterative state estimation that fits this problem well. We model the center’s drift as a random walk: each new position = previous position + random step. This gives a simple linear state-space model (the “drunkard’s walk” where each step is a small random offset ([A Gentle Introduction to the Random Walk for Times Series Forecasting with Python - MachineLearningMastery.com](https://www.machinelearningmastery.com/gentle-introduction-random-walk-times-series-forecasting-python/#:~:text=A%20random%20walk%20is%20different,previous%20value%20in%20the%20sequence))). The Kalman filter maintains a running estimate of the center position and updates it whenever a new measurement (computed center) is available. Importantly, it can propagate (predict) the center during frames where no measurement is taken, effectively *interpolating* over 10, 100, or 1000 frame gaps. When a new computed center comes in, the filter corrects its estimate by blending the prediction with the noisy observation, weighting them by their uncertainties. This sequential **predict-update** process is optimal under Gaussian noise assumptions and provides not only an estimate but also an uncertainty (error covariance) for the predicted center ([](https://arxiv.org/pdf/2007.03238#:~:text=Kalman%20filters%20,However%2C%20the%20major%20drawback%20of)). In our context, the filter’s process model would have a small process noise (to allow slow random drift) and the measurement model accounts for noise in the computed center. By adjusting these noise parameters, the filter can “trust” predictions more during stable periods and rely on measurements when large deviations occur.  

- **Advantages:** The Kalman filter is computationally light and *online*: it updates the estimate in real time with fixed cost per step ([](https://arxiv.org/pdf/2007.03238#:~:text=Kalman%20filters%20,However%2C%20the%20major%20drawback%20of)). It naturally handles irregular measurements – if you only measure every *N* frames, it simply performs *N-1* prediction steps in between, and the uncertainty grows in a quantified way. For a true random walk with Gaussian noise, Kalman filtering is statistically optimal, yielding the minimum mean-square error estimate. It also provides a built-in uncertainty estimate, which can inform whether the prediction is becoming unreliable (and thus when a new measurement is critical).  
- **Limitations:** A standard Kalman filter assumes Gaussian noise and no gross outliers. In practice, if one of the computed center data points is a wild outlier, the filter will **mis-correct** and could significantly stray, taking many subsequent frames to recover ([](https://arxiv.org/pdf/2007.03238#:~:text=whether%20that%20observation%20is%20anomalous,predictions%20for%20many%20future%20time)). Tuning is required: one must set the process noise variance (how much the drift wanders each frame) and measurement noise variance. If these are mis-specified, the filter might either over-smooth (ignoring real shifts) or over-fit the noisy observations. Additionally, Kalman filters are limited to linear models (or require extension for nonlinear cases), though a random walk is linear.  

# Robust Outlier-Resistant Filtering  
Real data often violates the Kalman filter’s assumptions due to **heavy-tailed noise or outliers**. To handle “mostly noise with occasional extreme outliers,” filtering algorithms need to be robust. Several statistical techniques extend or modify Kalman filtering to resist outliers. One approach is to assume a **heavy-tailed noise distribution** for measurements instead of Gaussian. For example, using a Student-*t* distribution for observation errors gives less weight to outlier residuals, preventing them from completely skewing the estimate ([](https://arxiv.org/pdf/2007.03238#:~:text=heavy%20tailed%20additive%20outliers,stat.ME%5D%207%20Jul%202020)). Another approach is to apply **M-estimation** principles to the filter: using a Huber loss or similar on the innovation (residual) so that large deviations are down-weighted ([](https://arxiv.org/pdf/2007.03238#:~:text=heavy%20tailed%20additive%20outliers,stat.ME%5D%207%20Jul%202020)). In practice, this can be implemented by iteratively reweighting the filter update or by inflation of the measurement noise covariance when a residual is suspiciously large ([](https://arxiv.org/pdf/2007.03238#:~:text=such%20filters%20include%20,A%20few)). A simple heuristic: if the new computed center is too far from the prediction (beyond some sigma threshold), treat it as an outlier by increasing its supposed noise – the filter will then discount that observation ([](https://arxiv.org/pdf/2007.03238#:~:text=such%20filters%20include%20,A%20few)). 

More advanced solutions combine **robust statistics with filtering**. For instance, the **KalmanSAC** algorithm (Kalman + RANSAC) uses a Random Sample Consensus approach at each update: it effectively tests subsets of past measurements to identify inliers, enabling the filter to update on a consensus of reliable points rather than a single potentially faulty observation. This method extends RANSAC into the time domain and can tolerate a high fraction of outliers in the data ([](https://ftp.cs.ucla.edu/tech-report/2005-reports/050011.pdf#:~:text=motion%20%28SFM%29,algorithms%20in%20a%20sampling%20framework)). Similarly, researchers have proposed particle-filter hybrids (e.g. CE-BASS) that mix Kalman updates with particle-based proposals to handle outliers and even multimodal uncertainties ([](https://arxiv.org/pdf/2007.03238#:~:text=ABSTRACT%20In%20this%20paper%2C%20we,the%20observations%2C%20such%20as%20trend)). These particle-assisted filters can “re-sample” past state hypotheses if an outlier caused a deviation, thus recovering from mis-tracks that a standard Kalman filter would persist with ([](https://arxiv.org/pdf/2007.03238#:~:text=ABSTRACT%20In%20this%20paper%2C%20we,the%20observations%2C%20such%20as%20trend)). 

- **Advantages:** Robust filtering methods greatly improve resilience to bad data. Heavy-tailed models and Huber-type updates allow the estimator to **ignore extreme outliers** instead of being dragged by them ([](https://arxiv.org/pdf/2007.03238#:~:text=whether%20that%20observation%20is%20anomalous,predictions%20for%20many%20future%20time)) ([](https://arxiv.org/pdf/2007.03238#:~:text=heavy%20tailed%20additive%20outliers,stat.ME%5D%207%20Jul%202020)). They can handle situations where, say, 5% of the frames produce wildly incorrect centers – a standard Kalman filter might be derailed by those, whereas a robust filter will largely reject them. Hybrid approaches like KalmanSAC can even cope with a *large proportion* of outliers (well above 50%) by relying on consensus from whichever measurements are consistent ([](https://ftp.cs.ucla.edu/tech-report/2005-reports/050011.pdf#:~:text=motion%20%28SFM%29,algorithms%20in%20a%20sampling%20framework)). Overall, these methods maintain a more stable track in the presence of anomalies.  
- **Limitations:** The added robustness often comes at the cost of **increased complexity or computation**. Heavy-tailed filters may require iterative re-estimation at each step (no longer closed-form) or sampling methods, which can slow down the real-time performance. Methods like KalmanSAC that sample multiple hypotheses or use RANSAC logic have higher computational overhead and may require careful parameter tuning (e.g. how many samples, what threshold defines an inlier). If outliers are infrequent, a simpler filter might suffice; robust methods shine in extreme conditions but can be overkill otherwise. Additionally, designing a robust filter requires assumptions about the nature of outliers (e.g. how heavy-tailed, or how to threshold), and if those assumptions are wrong, performance can suffer.  

# Particle Filters for Nonlinear Tracking  
A **particle filter** (sequential Monte Carlo method) is another iterative approach that can refine a drift estimate, especially useful if the drift model or noise characteristics are complex (non-Gaussian) or if we suspect multimodal uncertainty. It represents the unknown center position with a set of samples (particles) rather than a single Gaussian estimate. Each particle propagates according to the random walk model (with process noise), and when a new center measurement is available, the particles are weighted by how well they agree with that observation. The filter then resamples particles to concentrate around the more likely states. In essence, a particle filter can approximate the optimal Bayesian solution for arbitrary distributions. In our scenario, most of the time the particle cloud will spread out slowly during the prediction-only frames (reflecting growing uncertainty of drift), and when a noisy measurement comes in, the filter can **bimodally** handle the possibility that the measurement was an outlier (it might keep some particles near the predicted location and some near the measurement, until the next data resolves the ambiguity). This flexibility makes particle filters inherently robust to outliers in a way: a wild observation might simply get low weight and not eliminate the other hypotheses. In fact, advanced particle filter variants explicitly model outliers by using mixture distributions ([](https://arxiv.org/pdf/2007.03238#:~:text=ABSTRACT%20In%20this%20paper%2C%20we,the%20observations%2C%20such%20as%20trend)) or adding an “outlier state” particle that ignores certain observations. Particle filters have been widely used in object tracking and can handle the “sparse update” case by naturally continuing to propagate particles when no observations are present.  

- **Advantages:** Particle filters are very **flexible**. They can model non-Gaussian uncertainties and recover from erratic measurements because they don’t commit to a single state estimate until evidence accumulates. If the true drift has random jumps or the measurement noise has long tails, a particle filter can still track by allocating some particles to the unexpected but possible states. They also work with highly nonlinear dynamics or measurement models (though our drift is simple linear motion). In the context of sparse noisy data, a particle filter can maintain a distribution over the unknown center during long gaps, and when an observation arrives, it inherently performs a form of outlier check by weighting – if the observation is inconsistent with the prior distribution, it won’t fully override the estimate, unless it’s supported by multiple future observations.  
- **Limitations:** The main drawback is **computational cost**. To get a stable estimate, especially in 2D or 3D drift tracking, one might need hundreds or thousands of particles. This could be expensive if we’re trying to avoid computations in the first place (though computing the center itself is the expensive part; the particle filter update is usually lighter than doing an image computation). Also, particle filters can suffer from particle degeneracy (all particles collapsing) if not enough particles are used or if the noise is underestimated. They require careful tuning of the number of particles and noise models. In largely linear-Gaussian scenarios, a Kalman filter (perhaps with robust tweak) can achieve similar results much more efficiently ([Kalman Filter. A way to detect and remove outliers - Medium](https://medium.com/blogyuxiglobal/kalman-filter-the-way-to-remove-outliers-bb6aa616788e#:~:text=Kalman%20Filter,selection%20and%20outliers%20detection%2Frejection%20mechanism)). Thus, particle filters are best reserved for cases where simpler filters break down – e.g. highly non-Gaussian outlier scenarios or if the drift model were nonlinear (not the case for a pure random walk).  

# Iterative Curve-Fitting and Smoothing  
An alternative to state-space filtering is to treat the drift trajectory as a curve fitting problem, updating the fit as new frames come in. **Iterative smoothing** methods can be used to refine the estimated path of the drifting center, especially useful in offline analysis or when one wants a smoothed trajectory through noisy data points. One common technique is **locally weighted regression (LOESS/LOWESS)** with robustness against outliers. The idea is to fit a simple curve (like a low-order polynomial or even a straight line) through the center positions within a moving time window, giving more weight to closer points in time. A robust LOESS implementation does this fitting iteratively: first fit a curve to all data in the window, then identify points with large residuals (outliers relative to the fit) and reduce their weights or exclude them, then re-fit ([Robust fitting and smoothing](https://www.st-andrews.ac.uk/~wjh/dataview/tutorials/robust-fitting.html#:~:text=Robust%20polynomial%20fitting%20is%20implemented,and%206%20is%20a%20typical)) ([Robust fitting and smoothing](https://www.st-andrews.ac.uk/~wjh/dataview/tutorials/robust-fitting.html#:~:text=threshold%20value,to%20produce%20a%20good%20fit)). By repeating this, the influence of outliers on the smoothed curve is minimized ([Robust fitting and smoothing](https://www.st-andrews.ac.uk/~wjh/dataview/tutorials/robust-fitting.html#:~:text=Neither%20MA%20nor%20LP%20are,of%20outliers%20to%20the%20fit)). This produces a smoothly varying center position that ignores spikes due to bad frames. The procedure can operate on sparse data (it just sees whatever points are available in each window) and inherently fills the gaps by interpolation.  

Another approach is **iterative polynomial fitting** for the entire drift trajectory. For example, one could fit a low-degree polynomial or spline to all measured centers (every 100th frame perhaps), then remove or downweight points with large fitting errors, and iterate. This is essentially an **iteratively reweighted least squares** (IRLS) approach, where extreme outliers get zero weight after a couple of iterations ([Robust fitting and smoothing](https://www.st-andrews.ac.uk/~wjh/dataview/tutorials/robust-fitting.html#:~:text=Robust%20polynomial%20fitting%20is%20implemented,and%206%20is%20a%20typical)). If the drift is long and not well represented by a single polynomial, one might fit it piecewise or use splines (with a smoothing penalty). Spline smoothing can also incorporate robustness by limiting the influence of outliers on the least-squares criterion (using techniques like a bisquare loss). Additionally, a simpler method is a **moving median or percentile filter**: at each frame (or block of frames), take the median of recent center measurements to filter out transient spikes. A median filter is highly robust to isolated outliers (since median ignores extreme values), though it may not produce as smooth a trajectory as LOESS or Kalman smoothing.  

- **Advantages:** These regression-based methods are intuitive and make minimal assumptions about underlying physics – they treat the drift as an unknown smooth curve and let the data speak. Robust smoothing (like LOESS with outlier rejection) can *clean up noise and discard anomalies*, yielding a clear estimated path ([Robust fitting and smoothing](https://www.st-andrews.ac.uk/~wjh/dataview/tutorials/robust-fitting.html#:~:text=Neither%20MA%20nor%20LP%20are,of%20outliers%20to%20the%20fit)). They are also flexible with respect to sampling: irregularly spaced data points are fine, and you can incorporate as many or as few points as are available. Moreover, they can be applied in an **offline batch** manner to refine the entire trajectory after data collection, which can achieve higher accuracy than an online method that only looks one step ahead. For instance, a robust spline fit could use all frames to best estimate the drift (this is akin to a Kalman smoother which uses forward-backward passes).  
- **Limitations:** Pure curve-fitting approaches typically do not provide a predictive model – they smooth the observed data but don’t inherently model the random-walk dynamics. If used online, they often introduce lag (since a window or batch of points is needed to fit). In a real-time context, LOESS might need a buffer of future points, or if used causally, it becomes similar to a moving average with less effectiveness on the latest point. Additionally, one must choose parameters like the window size or polynomial degree; too small a window and the fit will be noisy, too large and it won’t adapt to changes. A drawback compared to Kalman filters is the lack of an uncertainty estimate – one gets a best-fit curve but not the confidence in position at a given frame. Lastly, while robust fitting handles outliers better than simple least-squares, it may struggle if outliers constitute a large fraction of the data (e.g., if more than half the samples in a window are bad, even the median can fail). These methods also can’t easily distinguish process noise from measurement noise – they just smooth everything, potentially smoothing out real rapid movements if those occur.  

# Gaussian Process Regression for Sparse Noisy Data  
Gaussian Process (GP) regression offers a Bayesian curve-fitting framework that is well-suited to sparse and noisy observations. We can treat the drifting center’s trajectory as a random function drawn from a GP with a covariance structure that reflects the random walk nature (for example, a Wiener process kernel or Matern kernel for smoothness). By feeding the GP the measured centers (e.g., one every 100 frames), it will produce a posterior mean function that interpolates between those points and a confidence band representing uncertainty. Importantly, GPs naturally handle irregular sampling – the inference only uses the actual observation times. They also excel at **sparse data interpolation**, effectively filling in the gaps in a principled way. A GP with a correctly chosen kernel can act much like an optimal smoother for the drift. For instance, a GP equivalent to a random walk model would predict a straight-line interpolation between points but with increasing uncertainty further from observations. One can update the GP iteratively as new points come (online learning for GPs is possible via recursive formulas or approximation since certain kernels lead to state-space forms, essentially Kalman filters under the hood).  

To address noise and outliers, one can use a **robust likelihood** in GP regression. Standard GPs assume Gaussian observation noise, which makes them susceptible to outliers (just like Kalman). But researchers have developed GP regression with heavy-tailed noise models, such as a Student-*t* likelihood ([[PDF] Robust Gaussian Process Regression with a Student-t Likelihood](https://jmlr.csail.mit.edu/papers/volume12/jylanki11a/jylanki11a.pdf#:~:text=,concave)) or Huber loss for the residuals ([[PDF] A robust approach to Gaussian process implementation](https://ascmo.copernicus.org/articles/10/143/2024/ascmo-10-143-2024.pdf#:~:text=Specifically%2C%20we%20introduce%20a%20novel,effectively%20accounts%20for%20outliers)). These approaches downweight the influence of outlier points on the fitted function. In practice, implementing a robust GP might involve approximate inference (since a non-Gaussian likelihood breaks the simple formulas), but methods like variational inference or expectation propagation can handle it ([[PDF] Robust Gaussian Process Regression with a Student-t Likelihood](https://jmlr.csail.mit.edu/papers/volume12/jylanki11a/jylanki11a.pdf#:~:text=,concave)). The end result is a smoothed trajectory that is learned from sparse, noisy data, with outliers effectively ignored as they have low likelihood under the heavy-tailed model. GPs also allow *hyperparameter adaptation*: one can learn the drift’s variance or time correlation from the data via evidence maximization, which is an iterative refinement of the fit in a statistical sense. 

- **Advantages:** GPs provide a very *principled and flexible* approach. They not only give a best-fit line but also quantify uncertainty at every point (useful to decide if we need more samples). They cope with widely spaced data and uneven intervals seamlessly. With appropriate kernels, they can model the expected smoothness or roughness of the drift (random walk is a rougher process, but one can incorporate any prior knowledge about drift continuity). Robust GP models further ensure that outliers don’t spoil the estimation ([[PDF] Robust Gaussian Process Regression with a Student-t Likelihood](https://jmlr.csail.mit.edu/papers/volume12/jylanki11a/jylanki11a.pdf#:~:text=,concave)). Another benefit is that GPs can **interpolate and extrapolate** in a Bayesian optimal way; for example, if you have two reliable center measurements 1000 frames apart, the GP essentially performs an optimal interpolation between them, and can even extrapolate beyond the last measurement with an uncertainty that grows. This is similar to a Kalman smoother’s result but achieved with a single batch computation given all data.  
- **Limitations:** A key issue with GPs is computational scalability. Naively, GP regression has cubic complexity in the number of data points, so if you eventually use hundreds or thousands of measured centers, it can become slow (though sparse GP approximations or using state-space representation can mitigate this). Online updating of GPs without full recomputation requires more complex implementations (like tracking sufficient statistics or using approximations), whereas Kalman filters natively update online. Additionally, choosing the right kernel and noise model is crucial – a mismatched kernel (e.g., assuming too smooth of a function) could underfit the drift, while assuming a random walk kernel might overfit noise if not regularized. Robust likelihood GP methods introduce additional tuning (e.g., degrees of freedom for Student-*t* noise) and usually require iterative techniques that are not as straightforward to implement as a Kalman filter. In summary, while GPs are powerful, they may be overkill if a simpler filter suffices, and their elegance comes with computational cost that needs to be managed for real-time use.  

**References:** The strategies above are grounded in well-established statistical methods for state estimation and curve fitting. Event-triggered and adaptive measurement techniques for Kalman filters have been shown to maintain accuracy while greatly reducing update rates ([Event-Triggered Kalman Filter and Its Performance Analysis](https://www.mdpi.com/1424-8220/23/4/2202#:~:text=accurately%20set,effectiveness%20of%20the%20proposed%20algorithm)) ([Event-Triggered Kalman Filter and Its Performance Analysis](https://www.mdpi.com/1424-8220/23/4/2202#:~:text=As%20early%20as%201983%2C%20Ho,sensor%20nodes%20that%20can%20be)). Standard Kalman filtering is a natural choice for random-walk models but must be augmented for robustness ([](https://arxiv.org/pdf/2007.03238#:~:text=whether%20that%20observation%20is%20anomalous,predictions%20for%20many%20future%20time)) ([](https://arxiv.org/pdf/2007.03238#:~:text=heavy%20tailed%20additive%20outliers,stat.ME%5D%207%20Jul%202020)). Numerous works address robust Kalman filtering, from using heavy-tailed noise models to Huber-weighted updates ([](https://arxiv.org/pdf/2007.03238#:~:text=heavy%20tailed%20additive%20outliers,stat.ME%5D%207%20Jul%202020)), and even RANSAC-style consensus approaches for dynamic systems ([](https://ftp.cs.ucla.edu/tech-report/2005-reports/050011.pdf#:~:text=motion%20%28SFM%29,algorithms%20in%20a%20sampling%20framework)). Particle filters provide a non-parametric alternative that can handle outliers and non-Gaussian behaviors ([](https://arxiv.org/pdf/2007.03238#:~:text=ABSTRACT%20In%20this%20paper%2C%20we,the%20observations%2C%20such%20as%20trend)). On the smoothing side, techniques like LOWESS and IRLS have long been used for robust trend estimation ([Robust fitting and smoothing](https://www.st-andrews.ac.uk/~wjh/dataview/tutorials/robust-fitting.html#:~:text=Robust%20polynomial%20fitting%20is%20implemented,and%206%20is%20a%20typical)) ([Robust fitting and smoothing](https://www.st-andrews.ac.uk/~wjh/dataview/tutorials/robust-fitting.html#:~:text=Neither%20MA%20nor%20LP%20are,of%20outliers%20to%20the%20fit)). Finally, Gaussian Process regression is a modern tool for interpolating sparse time-series data with uncertainty quantification, and recent research has made GPs more robust to outliers via modified likelihood functions ([[PDF] Robust Gaussian Process Regression with a Student-t Likelihood](https://jmlr.csail.mit.edu/papers/volume12/jylanki11a/jylanki11a.pdf#:~:text=,concave)). Each of these algorithms can be tailored to iteratively refine the drift estimation under the constraints given, and often a combination (e.g. a Kalman filter with event-triggered updates and robust outlier rejection) will yield the best results for tracking a noisy drifting center.