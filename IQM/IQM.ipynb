{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated IQM Evaluation of Stream Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automate_evaluation import automate_evaluation\n",
    "\n",
    "stream_file_folder = \"/home/bubl3932/files/UOX1/UOX1_original_IQM_v3\"   # Folder with stream file results from indexamajig. \n",
    "                                                                        # Note that all stream files in the folder will be processed.\n",
    "\n",
    "weights_list = [\n",
    "    (1, 1, 1, 1, 1, 1)\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Each weight corresponds to one of the six metrics used in calculating the combined IQM value.\n",
    "The combined IQM is computed by first normalizing each metric across all stream files, then \n",
    "multiplying each normalized metric by its assigned weight, and finally summing the results.\n",
    "The order (or keys) of the weights must match the following metrics:\n",
    "\n",
    "- 'weighted_rmsd'\n",
    "- 'fraction_outliers'\n",
    "- 'length_deviation'\n",
    "- 'angle_deviation'\n",
    "- 'peak_ratio'\n",
    "- 'percentage_indexed'\n",
    "\n",
    "Multiple weight combinations can be specified if needed.\n",
    "\"\"\"\n",
    "\n",
    "automate_evaluation(stream_file_folder, weights_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Histogram of IQM Values with Normal Distribution and an Eventual Cut-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm  # optional, if you want to plot the fitted PDF\n",
    "\n",
    "# Load your data defined in previous cell\n",
    "weights_string = \"_\".join(map(str, weights_list[0]))\n",
    "csv_path = f\"{stream_file_folder}/metric_values_IQM_{weights_string}.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Group by event_number and get the minimum combined_metric for each event.\n",
    "grouped_series = df.groupby(\"event_number\")[\"combined_metric\"].min()\n",
    "\n",
    "# Compute statistics of the combined_metric\n",
    "mean_metric = grouped_series.mean()\n",
    "std_metric = grouped_series.std()\n",
    "std_multiplier = 1.0  # you can change this multiplier to adjust the cutoff threshold\n",
    "\n",
    "# Define cutoff as mean + (std_multiplier * standard deviation)\n",
    "cutoff = mean_metric + std_multiplier * std_metric\n",
    "print(f\"Mean combined_metric: {mean_metric:.4f}\")\n",
    "print(f\"Standard deviation: {std_metric:.4f}\")\n",
    "print(f\"Cutoff threshold (mean + {std_multiplier} std): {cutoff:.4f}\")\n",
    "\n",
    "# Identify events with a combined_metric above the cutoff threshold.\n",
    "cutoff_series = grouped_series[grouped_series > cutoff]\n",
    "cutoff_events = cutoff_series.index.tolist()\n",
    "cutoff_metrics = cutoff_series.values.tolist()\n",
    "\n",
    "cutoff_number = len(cutoff_events)\n",
    "\n",
    "print(f\"Event numbers ({cutoff_number}) and combined metric values above the cutoff threshold:\")\n",
    "for event, metric in zip(cutoff_events, cutoff_metrics):\n",
    "    print(f\"Event {event}: Combined Metric = {metric:.4f}\")\n",
    "\n",
    "# Optionally, write out the events that exceed the cutoff threshold to a CSV file.\n",
    "output_df = pd.DataFrame({\n",
    "    \"event_number\": cutoff_events,\n",
    "    \"combined_metric\": cutoff_metrics\n",
    "})\n",
    "output_csv_path = f\"{stream_file_folder}/cutoff_events_with_metric_values_IQM_{weights_string}.csv\"\n",
    "output_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nCut-off Event and their metrics have been written to: {output_csv_path}\")\n",
    "\n",
    "# Plot the histogram of the data with the cutoff threshold indicated.\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(grouped_series.values, bins=100, edgecolor='black', alpha=0.6, label='Data')\n",
    "plt.axvline(cutoff, color='red', linestyle='dashed', linewidth=2, label=f'Cutoff = {cutoff:.2f}')\n",
    "\n",
    "# Optionally, overlay the fitted normal distribution curve.\n",
    "x_values = np.linspace(grouped_series.min(), grouped_series.max(), 1000)\n",
    "pdf_values = norm.pdf(x_values, loc=mean_metric, scale=std_metric)\n",
    "# Scale the PDF to match the histogram\n",
    "bin_width = (grouped_series.max() - grouped_series.min()) / 100.0\n",
    "pdf_values_scaled = pdf_values * len(grouped_series) * bin_width\n",
    "plt.plot(x_values, pdf_values_scaled, color='green', linewidth=2, label='Normal Distribution Fit')\n",
    "\n",
    "plt.title(\"Histogram with Normal Distribution Cutoff\")\n",
    "plt.xlabel(\"Combined Metric\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Partialator for Stream File Merging (Merge the Merged Stream File :D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running partialator for stream file: /Users/xiaodong/Desktop/simulations/LTA/simulation-29/LTAsim_from_file_-512.5_-512.5.stream\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partialator Progress: 100%|██████████| 7/7 [00:25<00:00,  3.59s/Residuals]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partialator completed for stream file: /Users/xiaodong/Desktop/simulations/LTA/simulation-29/LTAsim_from_file_-512.5_-512.5.stream\n",
      "All done. Results are in: /Users/xiaodong/Desktop/simulations/LTA/simulation-29/LTAsim_from_file_-512.5_-512.5_merge_5_iter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from run_partialator_and_convert import run_partialator_and_convert\n",
    "\n",
    "stream_file = \"/Users/xiaodong/Desktop/simulations/LTA/simulation-29/LTAsim_from_file_-512.5_-512.5.stream\" # Stream file to be merged\n",
    "pointgroup = \"m-3m\"  # Point group of the crystal\n",
    "num_threads = 8\n",
    "iterations = 5\n",
    "\n",
    "output_dir = run_partialator_and_convert(\n",
    "    stream_file,\n",
    "    pointgroup=pointgroup,\n",
    "    num_threads=num_threads,\n",
    "    iterations=iterations,\n",
    ")\n",
    "\n",
    "if output_dir is not None:\n",
    "    print(\"All done. Results are in:\", output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyxem-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
